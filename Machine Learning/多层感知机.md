#多层感知机(Multiple layer perceptron machine)

#### 概念
感知机(perceptron)是二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取+1和-1 二值。感知机对于与输入空间(特征空间)中讲实力划分为正负两类的分离超平面，属于判别模型，感知机学习旨在求出将训练数据进行线性划分的分离超平面，为此，导入基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型。感知机学习算法具有简单而易实现得优点，分为原始形式和对偶形式。感知机预测是用学习得到得感知机模型对新的输入实例进行分类。感知机在1957年由Rosenblatt提出，是神经网络与支持向量机的基础。


#### 定义
- 假设输入空间(特征空间)是$\mathcal{X} \subseteq \mathbf{R}^{n}$, 输入$x \in \mathcal{X}$表示实力的特征向量，对应于输入空间(特征空间)的点

- 输出空间是$\mathcal{Y}=\{+1,-1\}$；输出$y \in \mathcal{y}$表示实例的类别。

- 由输入空间到输出空间的如下函数：$f(x)=\operatorname{sign}(w \cdot x+b)$

上述三个特征的模型称为感知机。其中，w和b为感知机模型参数，$w \in \mathbf{R}^{n}$叫做权值(weight)或者权值向量(weight vector)，$b \in R$, $w \cdot x$表示w和x的内积，sign是符号函数，即

${\operatorname{sign}(x)=\left\{\begin{array}{ll}+1, & x \geqslant 0 \\ -1, & x<0\end{array}\right.}$

感知机是一种线性分类模型，属于判别模型。感知机模型的假设空间是定义在特征空间中的所有线性分类模型(linear classification model)或者线性分类器(linear classifier)，即函数集合$\{f|f(x) = w \cdot x +b \}$。
#### 感知机的几何解释
有线性方程：
$w \cdot x +b =0$
对应于特征空间$\mathbf{R}^{n}$中的一个超平面S，其中w是超平面的法向量，b是超平面的截距。这个超平面将特征空间划分为两个部分，位于两部分的点(特征向量)分为正、负两类。因此超平面S称为分离超平面(separating hyperplane)，
![Aaron Swartz](https://github.com/GemZq/study_about_AI/raw/master/Machine%20Learning/images/20210706211009.png)
对于模型最后输出为大于0还是小于0，其实就是每个样本与超平面法向量的夹角的关系。
点积在参数向量上的投影为正则为正类，点积投影为负则为负类，点积投影为0则为决策边界(只有两向量互相垂直，点积投影为零，) 

感知机学习，由训练数据集(实例的特征向量及类别)
$T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}$
其中,$x_i \in \mathcal{X}=\mathbf{R}^{n}$，$y_{i} \in \mathcal{Y}=\{+1,-1\}, \quad i=1,2, \cdots, N$,求的感知机模型，即求得模型参数$w,b$。感知机预测，通过学习得到的感知机模型，对于新的输入实例给出其对应的输出类别。

#### 感知机学习策略
数据集的线性可分性
线性可分性的定义：
给定一个数据集$T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}$，
其中，$x_i in \mathcal{X}=\mathbf{R}^{n}$，$y_{i} \in \mathcal{Y}=\{+1,-1\}, \quad i=1,2, \cdots, N$，如果存在某个超平面S：$w\cdot x + b =0$能够将数据集的正实例点和负实例点完全正确地划分到超平面两侧，即对所有$y_i = +1$的实例$i$，有$w\cdot x_i +b >0$，对所有$y_i = -1$的实例$i$，有$w\cdot x_i + b < 0$，则称数据集T为线性可分数据集(linearly separable data set)；否则，称数据集T线性不可分。
感知机的学习策略
假设训练数据集是线性可分的，感知机学习的目标是求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面。为了找出这样的超平面，即确定感知机模型参数$w,b$，需要确定一个学习策略，即定义(经验)损失函数并将损失函数极小化。
损失函数的一个自然选择是误分类点的总数。但是，这样的损失函数不是参数$w,b$的连续可导函数，不易优化。损失函数的另一个选择是误分类点到超平面S的总距离，这是感知机所采用的。为此，首先写出输入空间$\mathcal{R}^{n}$中的任意一点$x_0$到超平面S的距离：  
$\frac{1}{\|w\|}\left|w \cdot x_{0}+b\right|$  
这里，$\|w\|$是$w$的$L_2$范数。
其次，对于误分类的数据$(x_i,y_i)$来说，
$-y_i(w\cdot x_i+b)>0$
成立。因为当$w\cdot x_i +b>0$时，$y_i=-1$，而$w\cdot x_i+b<0$，$y_i =+1$。因此，误分类点x_i到超平面S的距离是  
$-\frac{1}{\|w\|} y_{i}\left(w \cdot x_{i}+b\right)$  
这样，假设超平面S的误分类点集合为M，那么所有误分类点到超平面S的总距离为  
$-\frac{1}{\|w\|} \sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)$  
不考虑$\frac{1}{\|w\|}$，就得到感知机学习的损失函数。  
给定训练数据集
$T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}$其中，$x_{i} \in \mathcal{X}=\mathbf{R}^{n}, \quad y_{i} \in \mathcal{Y}=\{+1,-1\}, \quad i=1,2, \cdot s, N$，感知机$sign(w\cdot x + b)$学习的损失函数定义为  
$L(w, b)=-\sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)$  
其中M为误分类点的集合。这个损失函数就是感知机学习的经验风险函数。
显然，损失函数$L(w,b)$是非负的。如果没有误分类点，损失函数的值为0。而且，误分类点越少，误分类点距离超平面越近，损失函数值就越小。一个特定的样本点的损失函数：在误分类时参数$w,b$的线性函数，在正确分类时是0.因此，给定训练数据集T，损失函数$L(w,b)$是w,b的连续可导函数。
#### Recently research 
- Pay Attention to MLPs.[Paper](https://arxiv.org/pdf/2105.08050.pdf)
- MLP-Mixer: An all-MLP Architecture for Vision. [Paper](https://arxiv.org/pdf/2105.01601.pdf) [Code](https://github.com/google-research/vision_transformer/tree/linen) https://mp.weixin.qq.com/s/rFMfG-lc5oybXa3f11ie2g